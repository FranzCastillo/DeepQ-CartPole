{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T19:48:11.144598Z",
     "start_time": "2024-09-27T19:48:11.139065Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 22:17:56.470509: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 22:17:56.695954: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-27 22:17:56.905519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-27 22:17:57.074766: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-27 22:17:57.138411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-27 22:17:57.487140: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-27 22:17:59.589515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727475482.762751      20 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-27 22:18:02.763295: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2343] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T19:53:24.096096Z",
     "start_time": "2024-09-27T19:53:24.090976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99  # Discount factor\n",
    "\n",
    "NUM_EPISODES = 500  # Number of episodes to run\n",
    "\n",
    "BATCH_SIZE = 64  # Number of experiences to sample per training step\n",
    "MEMORY_SIZE = 10000  # Experience replay memory size\n",
    "\n",
    "LEARNING_RATE = 0.001  # Learning rate\n",
    "EPSILON = 1.0  # Initial exploration rate\n",
    "EPSILON_DECAY = 0.995  # Exploration decay rate per episode\n",
    "MIN_EPSILON = 0.01  # Minimum exploration rate\n",
    "\n",
    "MAX_ITERATIONS = 500  # Max steps per episode\n",
    "TARGET_UPDATE_FREQUENCY = 10  # Episodes between target network updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T19:48:26.816556Z",
     "start_time": "2024-09-27T19:48:26.809724Z"
    }
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"\n",
    "    Experience Replay Memory.\n",
    "    \n",
    "    Args:\n",
    "        max_memory (int): Maximum number of experiences the Memory can keep.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_memory):\n",
    "        self._samples = deque(maxlen=max_memory)\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        num_samples = min(num_samples, len(self._samples))\n",
    "        return rand.sample(self._samples, num_samples)\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEP Q-LEARNING AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T19:48:27.547270Z",
     "start_time": "2024-09-27T19:48:27.534958Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Learning Agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = MIN_EPSILON\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "        self.memory = Memory(MEMORY_SIZE)\n",
    "\n",
    "        # Main model (Q-network) and target model (target Q-network)\n",
    "        self.main_model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Creates a neural network model.\n",
    "        \n",
    "        Returns:\n",
    "            keras.models.Sequential: Neural network model.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the target Q-network with the weights of the main Q-network.\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Stores an experience in the replay memory.\n",
    "        \"\"\"\n",
    "        self.memory.add_sample((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using the epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (array): Current state.\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rand.randrange(self.action_size)\n",
    "        q_values = self.main_model.predict(np.array([state]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Trains the Q-network using a batch of experiences from the replay memory.\n",
    "        \"\"\"\n",
    "        if self.memory.num_samples < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states = np.array([sample[0] for sample in batch])\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_states = np.array([sample[3] for sample in batch])\n",
    "        dones = np.array([sample[4] for sample in batch])\n",
    "        \n",
    "        # Target Q-values prediction\n",
    "        next_qs = self.target_model.predict(next_states, verbose=0)\n",
    "        max_next_qs = np.max(next_qs, axis=1)\n",
    "        target_qs = rewards + (1 - dones) * self.gamma * max_next_qs\n",
    "\n",
    "        # Actual Q-values prediction\n",
    "        current_qs = self.main_model.predict(states, verbose=0)\n",
    "        for i, action in enumerate(actions):\n",
    "            current_qs[i][action] = target_qs[i]\n",
    "\n",
    "        # Q-network training\n",
    "        self.main_model.fit(states, current_qs, epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Loads a model from a file.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the file.\n",
    "        \"\"\"\n",
    "        self.main_model = load_model(name)\n",
    "        self.target_model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Saves the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the file.\n",
    "        \"\"\"\n",
    "        self.main_model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T19:48:27.975150Z",
     "start_time": "2024-09-27T19:48:27.961629Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_agent(env, agent):\n",
    "    \"\"\"\n",
    "    Trains the agent using Deep Q-Learning.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): Environment.\n",
    "        agent (DQLAgent): Agent.\n",
    "        \n",
    "    Returns:\n",
    "        DQLAgent: Trained agent.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(MAX_ITERATIONS):\n",
    "            # Render environment\n",
    "            # env.render()\n",
    "\n",
    "            # Action selection\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Environment interaction\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store experience in replay memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Experience replay\n",
    "            agent.replay()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Episodio {episode + 1}/{NUM_EPISODES}, Recompensa Total: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "        \n",
    "        # Target model update\n",
    "        if (episode + 1) % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "    \n",
    "    # Graph total rewards per episode\n",
    "    plt.plot(range(1, NUM_EPISODES + 1), episode_rewards, marker='o')\n",
    "    plt.title('Recompensa Total por Episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa Total')\n",
    "    plt.xticks(range(1, NUM_EPISODES + 1))\n",
    "    y_max = max(episode_rewards) * 1.1\n",
    "    plt.ylim(0, y_max)\n",
    "    plt.show()\n",
    "\n",
    "def test_agent(env, agent, episodes=5):\n",
    "    \"\"\"\n",
    "    Tests the agent in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): Environment.\n",
    "        agent (DQLAgent): Agent.\n",
    "        episodes (int): Number of episodes to run.\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            q_values = agent.main_model.predict(np.array([state]), verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                total_rewards.append(total_reward)\n",
    "                print(f\"Prueba Episodio {episode + 1}/{episodes}, Recompensa Total: {total_reward}\")\n",
    "                break\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Recompensa promedio en {episodes} pruebas: {avg_reward}\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:53:06.747103Z",
     "start_time": "2024-09-27T21:53:06.740719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:52:03.445427Z",
     "start_time": "2024-09-27T19:53:30.615282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1/500, Recompensa Total: 12.0, Epsilon: 1.00\n",
      "Episodio 2/500, Recompensa Total: 14.0, Epsilon: 0.99\n",
      "Episodio 3/500, Recompensa Total: 24.0, Epsilon: 0.99\n",
      "Episodio 4/500, Recompensa Total: 13.0, Epsilon: 0.99\n",
      "Episodio 5/500, Recompensa Total: 23.0, Epsilon: 0.98\n",
      "Episodio 6/500, Recompensa Total: 17.0, Epsilon: 0.98\n",
      "Episodio 7/500, Recompensa Total: 96.0, Epsilon: 0.97\n",
      "Episodio 8/500, Recompensa Total: 24.0, Epsilon: 0.97\n",
      "Episodio 9/500, Recompensa Total: 18.0, Epsilon: 0.96\n",
      "Episodio 10/500, Recompensa Total: 22.0, Epsilon: 0.96\n",
      "Episodio 11/500, Recompensa Total: 19.0, Epsilon: 0.95\n",
      "Episodio 12/500, Recompensa Total: 15.0, Epsilon: 0.95\n",
      "Episodio 13/500, Recompensa Total: 17.0, Epsilon: 0.94\n",
      "Episodio 14/500, Recompensa Total: 22.0, Epsilon: 0.94\n",
      "Episodio 15/500, Recompensa Total: 29.0, Epsilon: 0.93\n",
      "Episodio 16/500, Recompensa Total: 14.0, Epsilon: 0.93\n",
      "Episodio 17/500, Recompensa Total: 28.0, Epsilon: 0.92\n",
      "Episodio 18/500, Recompensa Total: 23.0, Epsilon: 0.92\n",
      "Episodio 19/500, Recompensa Total: 19.0, Epsilon: 0.91\n",
      "Episodio 20/500, Recompensa Total: 15.0, Epsilon: 0.91\n",
      "Episodio 21/500, Recompensa Total: 17.0, Epsilon: 0.90\n",
      "Episodio 22/500, Recompensa Total: 49.0, Epsilon: 0.90\n",
      "Episodio 23/500, Recompensa Total: 12.0, Epsilon: 0.90\n",
      "Episodio 24/500, Recompensa Total: 32.0, Epsilon: 0.89\n",
      "Episodio 25/500, Recompensa Total: 16.0, Epsilon: 0.89\n",
      "Episodio 26/500, Recompensa Total: 15.0, Epsilon: 0.88\n",
      "Episodio 27/500, Recompensa Total: 12.0, Epsilon: 0.88\n",
      "Episodio 28/500, Recompensa Total: 21.0, Epsilon: 0.87\n",
      "Episodio 29/500, Recompensa Total: 18.0, Epsilon: 0.87\n",
      "Episodio 30/500, Recompensa Total: 35.0, Epsilon: 0.86\n",
      "Episodio 31/500, Recompensa Total: 28.0, Epsilon: 0.86\n",
      "Episodio 32/500, Recompensa Total: 19.0, Epsilon: 0.86\n",
      "Episodio 33/500, Recompensa Total: 17.0, Epsilon: 0.85\n",
      "Episodio 34/500, Recompensa Total: 38.0, Epsilon: 0.85\n",
      "Episodio 35/500, Recompensa Total: 44.0, Epsilon: 0.84\n",
      "Episodio 36/500, Recompensa Total: 33.0, Epsilon: 0.84\n",
      "Episodio 37/500, Recompensa Total: 14.0, Epsilon: 0.83\n",
      "Episodio 38/500, Recompensa Total: 13.0, Epsilon: 0.83\n",
      "Episodio 39/500, Recompensa Total: 13.0, Epsilon: 0.83\n",
      "Episodio 40/500, Recompensa Total: 14.0, Epsilon: 0.82\n",
      "Episodio 41/500, Recompensa Total: 13.0, Epsilon: 0.82\n",
      "Episodio 42/500, Recompensa Total: 17.0, Epsilon: 0.81\n",
      "Episodio 43/500, Recompensa Total: 13.0, Epsilon: 0.81\n",
      "Episodio 44/500, Recompensa Total: 11.0, Epsilon: 0.81\n",
      "Episodio 45/500, Recompensa Total: 24.0, Epsilon: 0.80\n",
      "Episodio 46/500, Recompensa Total: 72.0, Epsilon: 0.80\n",
      "Episodio 47/500, Recompensa Total: 18.0, Epsilon: 0.79\n",
      "Episodio 48/500, Recompensa Total: 39.0, Epsilon: 0.79\n",
      "Episodio 49/500, Recompensa Total: 25.0, Epsilon: 0.79\n",
      "Episodio 50/500, Recompensa Total: 40.0, Epsilon: 0.78\n",
      "Episodio 51/500, Recompensa Total: 10.0, Epsilon: 0.78\n",
      "Episodio 52/500, Recompensa Total: 28.0, Epsilon: 0.77\n",
      "Episodio 53/500, Recompensa Total: 32.0, Epsilon: 0.77\n",
      "Episodio 54/500, Recompensa Total: 18.0, Epsilon: 0.77\n",
      "Episodio 55/500, Recompensa Total: 13.0, Epsilon: 0.76\n",
      "Episodio 56/500, Recompensa Total: 45.0, Epsilon: 0.76\n",
      "Episodio 57/500, Recompensa Total: 40.0, Epsilon: 0.76\n",
      "Episodio 58/500, Recompensa Total: 25.0, Epsilon: 0.75\n",
      "Episodio 59/500, Recompensa Total: 67.0, Epsilon: 0.75\n",
      "Episodio 60/500, Recompensa Total: 37.0, Epsilon: 0.74\n",
      "Episodio 61/500, Recompensa Total: 10.0, Epsilon: 0.74\n",
      "Episodio 62/500, Recompensa Total: 42.0, Epsilon: 0.74\n",
      "Episodio 63/500, Recompensa Total: 38.0, Epsilon: 0.73\n",
      "Episodio 64/500, Recompensa Total: 51.0, Epsilon: 0.73\n",
      "Episodio 65/500, Recompensa Total: 44.0, Epsilon: 0.73\n",
      "Episodio 66/500, Recompensa Total: 28.0, Epsilon: 0.72\n",
      "Episodio 67/500, Recompensa Total: 19.0, Epsilon: 0.72\n",
      "Episodio 68/500, Recompensa Total: 46.0, Epsilon: 0.71\n",
      "Episodio 69/500, Recompensa Total: 16.0, Epsilon: 0.71\n",
      "Episodio 70/500, Recompensa Total: 28.0, Epsilon: 0.71\n",
      "Episodio 71/500, Recompensa Total: 36.0, Epsilon: 0.70\n",
      "Episodio 72/500, Recompensa Total: 11.0, Epsilon: 0.70\n",
      "Episodio 73/500, Recompensa Total: 14.0, Epsilon: 0.70\n",
      "Episodio 74/500, Recompensa Total: 73.0, Epsilon: 0.69\n",
      "Episodio 75/500, Recompensa Total: 26.0, Epsilon: 0.69\n",
      "Episodio 76/500, Recompensa Total: 36.0, Epsilon: 0.69\n",
      "Episodio 77/500, Recompensa Total: 11.0, Epsilon: 0.68\n",
      "Episodio 78/500, Recompensa Total: 19.0, Epsilon: 0.68\n",
      "Episodio 79/500, Recompensa Total: 21.0, Epsilon: 0.68\n",
      "Episodio 80/500, Recompensa Total: 41.0, Epsilon: 0.67\n",
      "Episodio 81/500, Recompensa Total: 59.0, Epsilon: 0.67\n",
      "Episodio 82/500, Recompensa Total: 35.0, Epsilon: 0.67\n",
      "Episodio 83/500, Recompensa Total: 43.0, Epsilon: 0.66\n",
      "Episodio 84/500, Recompensa Total: 46.0, Epsilon: 0.66\n",
      "Episodio 85/500, Recompensa Total: 83.0, Epsilon: 0.66\n",
      "Episodio 86/500, Recompensa Total: 170.0, Epsilon: 0.65\n",
      "Episodio 87/500, Recompensa Total: 39.0, Epsilon: 0.65\n",
      "Episodio 88/500, Recompensa Total: 43.0, Epsilon: 0.65\n",
      "Episodio 89/500, Recompensa Total: 105.0, Epsilon: 0.64\n",
      "Episodio 90/500, Recompensa Total: 47.0, Epsilon: 0.64\n",
      "Episodio 91/500, Recompensa Total: 33.0, Epsilon: 0.64\n",
      "Episodio 92/500, Recompensa Total: 51.0, Epsilon: 0.63\n",
      "Episodio 93/500, Recompensa Total: 59.0, Epsilon: 0.63\n",
      "Episodio 94/500, Recompensa Total: 17.0, Epsilon: 0.63\n",
      "Episodio 95/500, Recompensa Total: 78.0, Epsilon: 0.62\n",
      "Episodio 96/500, Recompensa Total: 44.0, Epsilon: 0.62\n",
      "Episodio 97/500, Recompensa Total: 15.0, Epsilon: 0.62\n",
      "Episodio 98/500, Recompensa Total: 105.0, Epsilon: 0.61\n",
      "Episodio 99/500, Recompensa Total: 83.0, Epsilon: 0.61\n",
      "Episodio 100/500, Recompensa Total: 39.0, Epsilon: 0.61\n",
      "Episodio 101/500, Recompensa Total: 88.0, Epsilon: 0.61\n",
      "Episodio 102/500, Recompensa Total: 30.0, Epsilon: 0.60\n",
      "Episodio 103/500, Recompensa Total: 131.0, Epsilon: 0.60\n",
      "Episodio 104/500, Recompensa Total: 16.0, Epsilon: 0.60\n",
      "Episodio 105/500, Recompensa Total: 50.0, Epsilon: 0.59\n",
      "Episodio 106/500, Recompensa Total: 67.0, Epsilon: 0.59\n",
      "Episodio 107/500, Recompensa Total: 61.0, Epsilon: 0.59\n",
      "Episodio 108/500, Recompensa Total: 78.0, Epsilon: 0.58\n",
      "Episodio 109/500, Recompensa Total: 76.0, Epsilon: 0.58\n",
      "Episodio 110/500, Recompensa Total: 184.0, Epsilon: 0.58\n",
      "Episodio 111/500, Recompensa Total: 90.0, Epsilon: 0.58\n",
      "Episodio 112/500, Recompensa Total: 26.0, Epsilon: 0.57\n",
      "Episodio 113/500, Recompensa Total: 36.0, Epsilon: 0.57\n",
      "Episodio 114/500, Recompensa Total: 36.0, Epsilon: 0.57\n",
      "Episodio 115/500, Recompensa Total: 178.0, Epsilon: 0.56\n",
      "Episodio 116/500, Recompensa Total: 50.0, Epsilon: 0.56\n",
      "Episodio 117/500, Recompensa Total: 72.0, Epsilon: 0.56\n",
      "Episodio 118/500, Recompensa Total: 54.0, Epsilon: 0.56\n",
      "Episodio 119/500, Recompensa Total: 50.0, Epsilon: 0.55\n",
      "Episodio 120/500, Recompensa Total: 86.0, Epsilon: 0.55\n",
      "Episodio 121/500, Recompensa Total: 24.0, Epsilon: 0.55\n",
      "Episodio 122/500, Recompensa Total: 33.0, Epsilon: 0.55\n",
      "Episodio 123/500, Recompensa Total: 90.0, Epsilon: 0.54\n",
      "Episodio 124/500, Recompensa Total: 42.0, Epsilon: 0.54\n",
      "Episodio 125/500, Recompensa Total: 70.0, Epsilon: 0.54\n",
      "Episodio 126/500, Recompensa Total: 46.0, Epsilon: 0.53\n",
      "Episodio 127/500, Recompensa Total: 62.0, Epsilon: 0.53\n",
      "Episodio 128/500, Recompensa Total: 204.0, Epsilon: 0.53\n",
      "Episodio 129/500, Recompensa Total: 109.0, Epsilon: 0.53\n",
      "Episodio 130/500, Recompensa Total: 39.0, Epsilon: 0.52\n",
      "Episodio 131/500, Recompensa Total: 18.0, Epsilon: 0.52\n",
      "Episodio 132/500, Recompensa Total: 93.0, Epsilon: 0.52\n",
      "Episodio 133/500, Recompensa Total: 45.0, Epsilon: 0.52\n",
      "Episodio 134/500, Recompensa Total: 18.0, Epsilon: 0.51\n",
      "Episodio 135/500, Recompensa Total: 184.0, Epsilon: 0.51\n",
      "Episodio 136/500, Recompensa Total: 46.0, Epsilon: 0.51\n",
      "Episodio 137/500, Recompensa Total: 32.0, Epsilon: 0.51\n",
      "Episodio 138/500, Recompensa Total: 53.0, Epsilon: 0.50\n",
      "Episodio 139/500, Recompensa Total: 90.0, Epsilon: 0.50\n",
      "Episodio 140/500, Recompensa Total: 126.0, Epsilon: 0.50\n",
      "Episodio 141/500, Recompensa Total: 124.0, Epsilon: 0.50\n",
      "Episodio 142/500, Recompensa Total: 69.0, Epsilon: 0.49\n",
      "Episodio 143/500, Recompensa Total: 141.0, Epsilon: 0.49\n",
      "Episodio 144/500, Recompensa Total: 105.0, Epsilon: 0.49\n",
      "Episodio 145/500, Recompensa Total: 93.0, Epsilon: 0.49\n",
      "Episodio 146/500, Recompensa Total: 137.0, Epsilon: 0.48\n",
      "Episodio 147/500, Recompensa Total: 250.0, Epsilon: 0.48\n",
      "Episodio 148/500, Recompensa Total: 35.0, Epsilon: 0.48\n",
      "Episodio 149/500, Recompensa Total: 30.0, Epsilon: 0.48\n",
      "Episodio 150/500, Recompensa Total: 123.0, Epsilon: 0.47\n",
      "Episodio 151/500, Recompensa Total: 180.0, Epsilon: 0.47\n",
      "Episodio 152/500, Recompensa Total: 62.0, Epsilon: 0.47\n",
      "Episodio 153/500, Recompensa Total: 18.0, Epsilon: 0.47\n",
      "Episodio 154/500, Recompensa Total: 63.0, Epsilon: 0.46\n",
      "Episodio 155/500, Recompensa Total: 65.0, Epsilon: 0.46\n",
      "Episodio 156/500, Recompensa Total: 31.0, Epsilon: 0.46\n",
      "Episodio 157/500, Recompensa Total: 197.0, Epsilon: 0.46\n",
      "Episodio 158/500, Recompensa Total: 20.0, Epsilon: 0.46\n",
      "Episodio 159/500, Recompensa Total: 160.0, Epsilon: 0.45\n",
      "Episodio 160/500, Recompensa Total: 51.0, Epsilon: 0.45\n",
      "Episodio 161/500, Recompensa Total: 136.0, Epsilon: 0.45\n",
      "Episodio 162/500, Recompensa Total: 35.0, Epsilon: 0.45\n",
      "Episodio 163/500, Recompensa Total: 91.0, Epsilon: 0.44\n",
      "Episodio 164/500, Recompensa Total: 28.0, Epsilon: 0.44\n",
      "Episodio 165/500, Recompensa Total: 63.0, Epsilon: 0.44\n",
      "Episodio 166/500, Recompensa Total: 45.0, Epsilon: 0.44\n",
      "Episodio 167/500, Recompensa Total: 84.0, Epsilon: 0.44\n",
      "Episodio 168/500, Recompensa Total: 15.0, Epsilon: 0.43\n",
      "Episodio 169/500, Recompensa Total: 159.0, Epsilon: 0.43\n",
      "Episodio 170/500, Recompensa Total: 77.0, Epsilon: 0.43\n",
      "Episodio 171/500, Recompensa Total: 81.0, Epsilon: 0.43\n",
      "Episodio 172/500, Recompensa Total: 286.0, Epsilon: 0.42\n",
      "Episodio 173/500, Recompensa Total: 165.0, Epsilon: 0.42\n",
      "Episodio 174/500, Recompensa Total: 161.0, Epsilon: 0.42\n",
      "Episodio 175/500, Recompensa Total: 81.0, Epsilon: 0.42\n",
      "Episodio 176/500, Recompensa Total: 257.0, Epsilon: 0.42\n",
      "Episodio 177/500, Recompensa Total: 233.0, Epsilon: 0.41\n",
      "Episodio 178/500, Recompensa Total: 105.0, Epsilon: 0.41\n",
      "Episodio 179/500, Recompensa Total: 71.0, Epsilon: 0.41\n",
      "Episodio 180/500, Recompensa Total: 66.0, Epsilon: 0.41\n",
      "Episodio 181/500, Recompensa Total: 195.0, Epsilon: 0.41\n",
      "Episodio 182/500, Recompensa Total: 110.0, Epsilon: 0.40\n",
      "Episodio 183/500, Recompensa Total: 394.0, Epsilon: 0.40\n",
      "Episodio 184/500, Recompensa Total: 199.0, Epsilon: 0.40\n",
      "Episodio 185/500, Recompensa Total: 42.0, Epsilon: 0.40\n",
      "Episodio 186/500, Recompensa Total: 44.0, Epsilon: 0.40\n",
      "Episodio 187/500, Recompensa Total: 25.0, Epsilon: 0.39\n",
      "Episodio 188/500, Recompensa Total: 51.0, Epsilon: 0.39\n",
      "Episodio 189/500, Recompensa Total: 35.0, Epsilon: 0.39\n",
      "Episodio 190/500, Recompensa Total: 153.0, Epsilon: 0.39\n",
      "Episodio 191/500, Recompensa Total: 110.0, Epsilon: 0.39\n",
      "Episodio 192/500, Recompensa Total: 36.0, Epsilon: 0.38\n",
      "Episodio 193/500, Recompensa Total: 63.0, Epsilon: 0.38\n",
      "Episodio 194/500, Recompensa Total: 25.0, Epsilon: 0.38\n",
      "Episodio 195/500, Recompensa Total: 89.0, Epsilon: 0.38\n",
      "Episodio 196/500, Recompensa Total: 90.0, Epsilon: 0.38\n",
      "Episodio 197/500, Recompensa Total: 481.0, Epsilon: 0.37\n",
      "Episodio 198/500, Recompensa Total: 56.0, Epsilon: 0.37\n",
      "Episodio 199/500, Recompensa Total: 50.0, Epsilon: 0.37\n",
      "Episodio 200/500, Recompensa Total: 110.0, Epsilon: 0.37\n",
      "Episodio 201/500, Recompensa Total: 252.0, Epsilon: 0.37\n",
      "Episodio 202/500, Recompensa Total: 85.0, Epsilon: 0.37\n",
      "Episodio 203/500, Recompensa Total: 107.0, Epsilon: 0.36\n",
      "Episodio 204/500, Recompensa Total: 106.0, Epsilon: 0.36\n",
      "Episodio 205/500, Recompensa Total: 22.0, Epsilon: 0.36\n",
      "Episodio 206/500, Recompensa Total: 92.0, Epsilon: 0.36\n",
      "Episodio 207/500, Recompensa Total: 51.0, Epsilon: 0.36\n",
      "Episodio 208/500, Recompensa Total: 118.0, Epsilon: 0.35\n",
      "Episodio 209/500, Recompensa Total: 112.0, Epsilon: 0.35\n",
      "Episodio 210/500, Recompensa Total: 51.0, Epsilon: 0.35\n",
      "Episodio 211/500, Recompensa Total: 14.0, Epsilon: 0.35\n",
      "Episodio 212/500, Recompensa Total: 128.0, Epsilon: 0.35\n",
      "Episodio 213/500, Recompensa Total: 110.0, Epsilon: 0.35\n",
      "Episodio 214/500, Recompensa Total: 43.0, Epsilon: 0.34\n",
      "Episodio 215/500, Recompensa Total: 292.0, Epsilon: 0.34\n",
      "Episodio 216/500, Recompensa Total: 86.0, Epsilon: 0.34\n",
      "Episodio 217/500, Recompensa Total: 110.0, Epsilon: 0.34\n",
      "Episodio 218/500, Recompensa Total: 70.0, Epsilon: 0.34\n",
      "Episodio 219/500, Recompensa Total: 11.0, Epsilon: 0.34\n",
      "Episodio 220/500, Recompensa Total: 51.0, Epsilon: 0.33\n",
      "Episodio 221/500, Recompensa Total: 54.0, Epsilon: 0.33\n",
      "Episodio 222/500, Recompensa Total: 10.0, Epsilon: 0.33\n",
      "Episodio 223/500, Recompensa Total: 193.0, Epsilon: 0.33\n",
      "Episodio 224/500, Recompensa Total: 150.0, Epsilon: 0.33\n",
      "Episodio 225/500, Recompensa Total: 61.0, Epsilon: 0.33\n",
      "Episodio 226/500, Recompensa Total: 113.0, Epsilon: 0.32\n",
      "Episodio 227/500, Recompensa Total: 258.0, Epsilon: 0.32\n",
      "Episodio 228/500, Recompensa Total: 109.0, Epsilon: 0.32\n",
      "Episodio 229/500, Recompensa Total: 14.0, Epsilon: 0.32\n",
      "Episodio 230/500, Recompensa Total: 249.0, Epsilon: 0.32\n",
      "Episodio 231/500, Recompensa Total: 73.0, Epsilon: 0.32\n",
      "Episodio 232/500, Recompensa Total: 500.0, Epsilon: 0.31\n",
      "Episodio 233/500, Recompensa Total: 29.0, Epsilon: 0.31\n",
      "Episodio 234/500, Recompensa Total: 168.0, Epsilon: 0.31\n",
      "Episodio 235/500, Recompensa Total: 21.0, Epsilon: 0.31\n",
      "Episodio 236/500, Recompensa Total: 301.0, Epsilon: 0.31\n",
      "Episodio 237/500, Recompensa Total: 158.0, Epsilon: 0.31\n",
      "Episodio 238/500, Recompensa Total: 54.0, Epsilon: 0.30\n",
      "Episodio 239/500, Recompensa Total: 177.0, Epsilon: 0.30\n",
      "Episodio 240/500, Recompensa Total: 102.0, Epsilon: 0.30\n",
      "Episodio 241/500, Recompensa Total: 133.0, Epsilon: 0.30\n",
      "Episodio 242/500, Recompensa Total: 20.0, Epsilon: 0.30\n",
      "Episodio 243/500, Recompensa Total: 64.0, Epsilon: 0.30\n",
      "Episodio 244/500, Recompensa Total: 101.0, Epsilon: 0.30\n",
      "Episodio 245/500, Recompensa Total: 23.0, Epsilon: 0.29\n",
      "Episodio 246/500, Recompensa Total: 69.0, Epsilon: 0.29\n",
      "Episodio 247/500, Recompensa Total: 163.0, Epsilon: 0.29\n",
      "Episodio 248/500, Recompensa Total: 294.0, Epsilon: 0.29\n",
      "Episodio 249/500, Recompensa Total: 231.0, Epsilon: 0.29\n",
      "Episodio 250/500, Recompensa Total: 344.0, Epsilon: 0.29\n",
      "Episodio 251/500, Recompensa Total: 27.0, Epsilon: 0.29\n",
      "Episodio 252/500, Recompensa Total: 63.0, Epsilon: 0.28\n",
      "Episodio 253/500, Recompensa Total: 82.0, Epsilon: 0.28\n",
      "Episodio 254/500, Recompensa Total: 112.0, Epsilon: 0.28\n",
      "Episodio 255/500, Recompensa Total: 224.0, Epsilon: 0.28\n",
      "Episodio 256/500, Recompensa Total: 96.0, Epsilon: 0.28\n",
      "Episodio 257/500, Recompensa Total: 107.0, Epsilon: 0.28\n",
      "Episodio 258/500, Recompensa Total: 98.0, Epsilon: 0.28\n",
      "Episodio 259/500, Recompensa Total: 136.0, Epsilon: 0.27\n",
      "Episodio 260/500, Recompensa Total: 25.0, Epsilon: 0.27\n",
      "Episodio 261/500, Recompensa Total: 43.0, Epsilon: 0.27\n",
      "Episodio 262/500, Recompensa Total: 150.0, Epsilon: 0.27\n",
      "Episodio 263/500, Recompensa Total: 143.0, Epsilon: 0.27\n",
      "Episodio 264/500, Recompensa Total: 85.0, Epsilon: 0.27\n",
      "Episodio 265/500, Recompensa Total: 190.0, Epsilon: 0.27\n",
      "Episodio 266/500, Recompensa Total: 55.0, Epsilon: 0.26\n",
      "Episodio 267/500, Recompensa Total: 17.0, Epsilon: 0.26\n",
      "Episodio 268/500, Recompensa Total: 68.0, Epsilon: 0.26\n",
      "Episodio 269/500, Recompensa Total: 111.0, Epsilon: 0.26\n",
      "Episodio 270/500, Recompensa Total: 46.0, Epsilon: 0.26\n",
      "Episodio 271/500, Recompensa Total: 114.0, Epsilon: 0.26\n",
      "Episodio 272/500, Recompensa Total: 163.0, Epsilon: 0.26\n",
      "Episodio 273/500, Recompensa Total: 37.0, Epsilon: 0.26\n",
      "Episodio 274/500, Recompensa Total: 95.0, Epsilon: 0.25\n",
      "Episodio 275/500, Recompensa Total: 117.0, Epsilon: 0.25\n",
      "Episodio 276/500, Recompensa Total: 356.0, Epsilon: 0.25\n",
      "Episodio 277/500, Recompensa Total: 140.0, Epsilon: 0.25\n",
      "Episodio 278/500, Recompensa Total: 123.0, Epsilon: 0.25\n",
      "Episodio 279/500, Recompensa Total: 31.0, Epsilon: 0.25\n",
      "Episodio 280/500, Recompensa Total: 184.0, Epsilon: 0.25\n",
      "Episodio 281/500, Recompensa Total: 111.0, Epsilon: 0.25\n",
      "Episodio 282/500, Recompensa Total: 80.0, Epsilon: 0.24\n",
      "Episodio 283/500, Recompensa Total: 101.0, Epsilon: 0.24\n",
      "Episodio 284/500, Recompensa Total: 205.0, Epsilon: 0.24\n",
      "Episodio 285/500, Recompensa Total: 108.0, Epsilon: 0.24\n",
      "Episodio 286/500, Recompensa Total: 35.0, Epsilon: 0.24\n",
      "Episodio 287/500, Recompensa Total: 70.0, Epsilon: 0.24\n",
      "Episodio 288/500, Recompensa Total: 160.0, Epsilon: 0.24\n",
      "Episodio 289/500, Recompensa Total: 22.0, Epsilon: 0.24\n",
      "Episodio 290/500, Recompensa Total: 179.0, Epsilon: 0.23\n",
      "Episodio 291/500, Recompensa Total: 119.0, Epsilon: 0.23\n",
      "Episodio 292/500, Recompensa Total: 65.0, Epsilon: 0.23\n",
      "Episodio 293/500, Recompensa Total: 29.0, Epsilon: 0.23\n",
      "Episodio 294/500, Recompensa Total: 38.0, Epsilon: 0.23\n",
      "Episodio 295/500, Recompensa Total: 80.0, Epsilon: 0.23\n",
      "Episodio 296/500, Recompensa Total: 148.0, Epsilon: 0.23\n",
      "Episodio 297/500, Recompensa Total: 105.0, Epsilon: 0.23\n",
      "Episodio 298/500, Recompensa Total: 92.0, Epsilon: 0.23\n",
      "Episodio 299/500, Recompensa Total: 103.0, Epsilon: 0.22\n",
      "Episodio 300/500, Recompensa Total: 95.0, Epsilon: 0.22\n",
      "Episodio 301/500, Recompensa Total: 66.0, Epsilon: 0.22\n",
      "Episodio 302/500, Recompensa Total: 68.0, Epsilon: 0.22\n",
      "Episodio 303/500, Recompensa Total: 26.0, Epsilon: 0.22\n",
      "Episodio 304/500, Recompensa Total: 87.0, Epsilon: 0.22\n",
      "Episodio 305/500, Recompensa Total: 94.0, Epsilon: 0.22\n",
      "Episodio 306/500, Recompensa Total: 94.0, Epsilon: 0.22\n",
      "Episodio 307/500, Recompensa Total: 30.0, Epsilon: 0.22\n",
      "Episodio 308/500, Recompensa Total: 28.0, Epsilon: 0.21\n",
      "Episodio 309/500, Recompensa Total: 135.0, Epsilon: 0.21\n",
      "Episodio 310/500, Recompensa Total: 23.0, Epsilon: 0.21\n",
      "Episodio 311/500, Recompensa Total: 69.0, Epsilon: 0.21\n",
      "Episodio 312/500, Recompensa Total: 90.0, Epsilon: 0.21\n",
      "Episodio 313/500, Recompensa Total: 297.0, Epsilon: 0.21\n",
      "Episodio 314/500, Recompensa Total: 26.0, Epsilon: 0.21\n",
      "Episodio 315/500, Recompensa Total: 17.0, Epsilon: 0.21\n",
      "Episodio 316/500, Recompensa Total: 78.0, Epsilon: 0.21\n",
      "Episodio 317/500, Recompensa Total: 63.0, Epsilon: 0.21\n",
      "Episodio 318/500, Recompensa Total: 80.0, Epsilon: 0.20\n",
      "Episodio 319/500, Recompensa Total: 78.0, Epsilon: 0.20\n",
      "Episodio 320/500, Recompensa Total: 55.0, Epsilon: 0.20\n",
      "Episodio 321/500, Recompensa Total: 101.0, Epsilon: 0.20\n",
      "Episodio 322/500, Recompensa Total: 40.0, Epsilon: 0.20\n",
      "Episodio 323/500, Recompensa Total: 110.0, Epsilon: 0.20\n",
      "Episodio 324/500, Recompensa Total: 130.0, Epsilon: 0.20\n",
      "Episodio 325/500, Recompensa Total: 38.0, Epsilon: 0.20\n",
      "Episodio 326/500, Recompensa Total: 33.0, Epsilon: 0.20\n",
      "Episodio 327/500, Recompensa Total: 139.0, Epsilon: 0.20\n",
      "Episodio 328/500, Recompensa Total: 256.0, Epsilon: 0.19\n",
      "Episodio 329/500, Recompensa Total: 43.0, Epsilon: 0.19\n",
      "Episodio 330/500, Recompensa Total: 71.0, Epsilon: 0.19\n",
      "Episodio 331/500, Recompensa Total: 21.0, Epsilon: 0.19\n",
      "Episodio 332/500, Recompensa Total: 121.0, Epsilon: 0.19\n",
      "Episodio 333/500, Recompensa Total: 22.0, Epsilon: 0.19\n",
      "Episodio 334/500, Recompensa Total: 52.0, Epsilon: 0.19\n",
      "Episodio 335/500, Recompensa Total: 64.0, Epsilon: 0.19\n",
      "Episodio 336/500, Recompensa Total: 168.0, Epsilon: 0.19\n",
      "Episodio 337/500, Recompensa Total: 63.0, Epsilon: 0.19\n",
      "Episodio 338/500, Recompensa Total: 139.0, Epsilon: 0.18\n",
      "Episodio 339/500, Recompensa Total: 236.0, Epsilon: 0.18\n",
      "Episodio 340/500, Recompensa Total: 57.0, Epsilon: 0.18\n",
      "Episodio 341/500, Recompensa Total: 40.0, Epsilon: 0.18\n",
      "Episodio 342/500, Recompensa Total: 31.0, Epsilon: 0.18\n",
      "Episodio 343/500, Recompensa Total: 131.0, Epsilon: 0.18\n",
      "Episodio 344/500, Recompensa Total: 97.0, Epsilon: 0.18\n",
      "Episodio 345/500, Recompensa Total: 170.0, Epsilon: 0.18\n",
      "Episodio 346/500, Recompensa Total: 28.0, Epsilon: 0.18\n",
      "Episodio 347/500, Recompensa Total: 115.0, Epsilon: 0.18\n",
      "Episodio 348/500, Recompensa Total: 16.0, Epsilon: 0.18\n",
      "Episodio 349/500, Recompensa Total: 37.0, Epsilon: 0.17\n",
      "Episodio 350/500, Recompensa Total: 235.0, Epsilon: 0.17\n",
      "Episodio 351/500, Recompensa Total: 17.0, Epsilon: 0.17\n",
      "Episodio 352/500, Recompensa Total: 321.0, Epsilon: 0.17\n",
      "Episodio 353/500, Recompensa Total: 313.0, Epsilon: 0.17\n",
      "Episodio 354/500, Recompensa Total: 155.0, Epsilon: 0.17\n",
      "Episodio 355/500, Recompensa Total: 280.0, Epsilon: 0.17\n",
      "Episodio 356/500, Recompensa Total: 79.0, Epsilon: 0.17\n",
      "Episodio 357/500, Recompensa Total: 318.0, Epsilon: 0.17\n",
      "Episodio 358/500, Recompensa Total: 240.0, Epsilon: 0.17\n",
      "Episodio 359/500, Recompensa Total: 54.0, Epsilon: 0.17\n",
      "Episodio 360/500, Recompensa Total: 44.0, Epsilon: 0.17\n",
      "Episodio 361/500, Recompensa Total: 235.0, Epsilon: 0.16\n",
      "Episodio 362/500, Recompensa Total: 254.0, Epsilon: 0.16\n",
      "Episodio 363/500, Recompensa Total: 75.0, Epsilon: 0.16\n",
      "Episodio 364/500, Recompensa Total: 326.0, Epsilon: 0.16\n",
      "Episodio 365/500, Recompensa Total: 106.0, Epsilon: 0.16\n",
      "Episodio 366/500, Recompensa Total: 216.0, Epsilon: 0.16\n",
      "Episodio 367/500, Recompensa Total: 280.0, Epsilon: 0.16\n",
      "Episodio 368/500, Recompensa Total: 77.0, Epsilon: 0.16\n",
      "Episodio 369/500, Recompensa Total: 108.0, Epsilon: 0.16\n",
      "Episodio 370/500, Recompensa Total: 242.0, Epsilon: 0.16\n",
      "Episodio 371/500, Recompensa Total: 71.0, Epsilon: 0.16\n",
      "Episodio 372/500, Recompensa Total: 288.0, Epsilon: 0.16\n",
      "Episodio 373/500, Recompensa Total: 265.0, Epsilon: 0.15\n",
      "Episodio 374/500, Recompensa Total: 282.0, Epsilon: 0.15\n",
      "Episodio 375/500, Recompensa Total: 26.0, Epsilon: 0.15\n",
      "Episodio 376/500, Recompensa Total: 251.0, Epsilon: 0.15\n",
      "Episodio 377/500, Recompensa Total: 120.0, Epsilon: 0.15\n",
      "Episodio 378/500, Recompensa Total: 124.0, Epsilon: 0.15\n",
      "Episodio 379/500, Recompensa Total: 31.0, Epsilon: 0.15\n",
      "Episodio 380/500, Recompensa Total: 41.0, Epsilon: 0.15\n",
      "Episodio 381/500, Recompensa Total: 320.0, Epsilon: 0.15\n",
      "Episodio 382/500, Recompensa Total: 95.0, Epsilon: 0.15\n",
      "Episodio 383/500, Recompensa Total: 21.0, Epsilon: 0.15\n",
      "Episodio 384/500, Recompensa Total: 18.0, Epsilon: 0.15\n",
      "Episodio 385/500, Recompensa Total: 440.0, Epsilon: 0.15\n",
      "Episodio 386/500, Recompensa Total: 206.0, Epsilon: 0.15\n",
      "Episodio 387/500, Recompensa Total: 30.0, Epsilon: 0.14\n",
      "Episodio 388/500, Recompensa Total: 259.0, Epsilon: 0.14\n",
      "Episodio 389/500, Recompensa Total: 22.0, Epsilon: 0.14\n",
      "Episodio 390/500, Recompensa Total: 314.0, Epsilon: 0.14\n",
      "Episodio 391/500, Recompensa Total: 41.0, Epsilon: 0.14\n",
      "Episodio 392/500, Recompensa Total: 89.0, Epsilon: 0.14\n",
      "Episodio 393/500, Recompensa Total: 122.0, Epsilon: 0.14\n",
      "Episodio 394/500, Recompensa Total: 458.0, Epsilon: 0.14\n",
      "Episodio 395/500, Recompensa Total: 292.0, Epsilon: 0.14\n",
      "Episodio 396/500, Recompensa Total: 300.0, Epsilon: 0.14\n",
      "Episodio 397/500, Recompensa Total: 247.0, Epsilon: 0.14\n",
      "Episodio 398/500, Recompensa Total: 447.0, Epsilon: 0.14\n",
      "Episodio 399/500, Recompensa Total: 290.0, Epsilon: 0.14\n",
      "Episodio 400/500, Recompensa Total: 30.0, Epsilon: 0.14\n",
      "Episodio 401/500, Recompensa Total: 100.0, Epsilon: 0.13\n",
      "Episodio 402/500, Recompensa Total: 149.0, Epsilon: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x000001B4828CCB80>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\weakref.py\", line 369, in remove\n",
      "    def remove(k, selfref=ref(self)):\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 403/500, Recompensa Total: 288.0, Epsilon: 0.13\n",
      "Episodio 404/500, Recompensa Total: 118.0, Epsilon: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Agent\n",
    "agent = DQLAgent(state_size, action_size)\n",
    "\n",
    "# Train agent\n",
    "train_agent(env, agent)\n",
    "\n",
    "# Save agent\n",
    "agent.save(\"cartpole-dql.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model in the environment without rendering\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQLAgent(state_size, action_size)\n",
    "agent.load(\"cartpole-dql.h5\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
