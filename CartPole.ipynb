{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install numpy\n",
    "# !pip install torch\n",
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99  # Discount factor\n",
    "\n",
    "# !!! (Test value, should be changed to 100-500. -Diego)\n",
    "NUM_EPISODES = 5  # Number of episodes to run\n",
    "\n",
    "BATCH_SIZE = 64  # Number of experiences to sample per training step\n",
    "MEMORY_SIZE = 10000  # Experience replay memory size\n",
    "\n",
    "LEARNING_RATE = 0.001  # Learning rate\n",
    "EPSILON = 1.0  # Initial exploration rate\n",
    "EPSILON_DECAY = 0.995  # Exploration decay rate per episode\n",
    "MIN_EPSILON = 0.01  # Minimum exploration rate\n",
    "\n",
    "MAX_ITERATIONS = 500  # Max steps per episode\n",
    "TARGET_UPDATE_FREQUENCY = 10  # Episodes between target network updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    \"\"\"\n",
    "    Experience Replay Memory.\n",
    "    \n",
    "    Args:\n",
    "        max_memory (int): Maximum number of experiences the Memory can keep.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_memory):\n",
    "        self._samples = deque(maxlen=max_memory)\n",
    "\n",
    "    def add_sample(self, sample):\n",
    "        self._samples.append(sample)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        num_samples = min(num_samples, len(self._samples))\n",
    "        return rand.sample(self._samples, num_samples)\n",
    "\n",
    "    @property\n",
    "    def num_samples(self):\n",
    "        return len(self._samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEP Q-LEARNING AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Learning Agent.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_min = MIN_EPSILON\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.batch_size = BATCH_SIZE\n",
    "\n",
    "        self.memory = Memory(MEMORY_SIZE)\n",
    "\n",
    "        # Main model (Q-network) and target model (target Q-network)\n",
    "        self.main_model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Creates a neural network model.\n",
    "        \n",
    "        Returns:\n",
    "            keras.models.Sequential: Neural network model.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the target Q-network with the weights of the main Q-network.\n",
    "        \"\"\"\n",
    "        self.target_model.set_weights(self.main_model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Stores an experience in the replay memory.\n",
    "        \"\"\"\n",
    "        self.memory.add_sample((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action using the epsilon-greedy policy.\n",
    "        \n",
    "        Args:\n",
    "            state (array): Current state.\n",
    "            \n",
    "        Returns:\n",
    "            int: Selected action.\n",
    "        \"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return rand.randrange(self.action_size)\n",
    "        q_values = self.main_model.predict(np.array([state]), verbose=0)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Trains the Q-network using a batch of experiences from the replay memory.\n",
    "        \"\"\"\n",
    "        if self.memory.num_samples < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        states = np.array([sample[0] for sample in batch])\n",
    "        actions = np.array([sample[1] for sample in batch])\n",
    "        rewards = np.array([sample[2] for sample in batch])\n",
    "        next_states = np.array([sample[3] for sample in batch])\n",
    "        dones = np.array([sample[4] for sample in batch])\n",
    "        \n",
    "        # Target Q-values prediction\n",
    "        next_qs = self.target_model.predict(next_states, verbose=0)\n",
    "        max_next_qs = np.max(next_qs, axis=1)\n",
    "        target_qs = rewards + (1 - dones) * self.gamma * max_next_qs\n",
    "\n",
    "        # Actual Q-values prediction\n",
    "        current_qs = self.main_model.predict(states, verbose=0)\n",
    "        for i, action in enumerate(actions):\n",
    "            current_qs[i][action] = target_qs[i]\n",
    "\n",
    "        # Q-network training\n",
    "        self.main_model.fit(states, current_qs, epochs=1, verbose=0)\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"\n",
    "        Loads a model from a file.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the file.\n",
    "        \"\"\"\n",
    "        self.main_model = load_model(name)\n",
    "        self.target_model = load_model(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        \"\"\"\n",
    "        Saves the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            name (str): Name of the file.\n",
    "        \"\"\"\n",
    "        self.main_model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent):\n",
    "    \"\"\"\n",
    "    Trains the agent using Deep Q-Learning.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): Environment.\n",
    "        agent (DQLAgent): Agent.\n",
    "        \n",
    "    Returns:\n",
    "        DQLAgent: Trained agent.\n",
    "    \"\"\"\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(MAX_ITERATIONS):\n",
    "            # Render environment\n",
    "            # env.render()\n",
    "\n",
    "            # Action selection\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Environment interaction\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Store experience in replay memory\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Experience replay\n",
    "            agent.replay()\n",
    "\n",
    "            if done or truncated:\n",
    "                print(f\"Episodio {episode + 1}/{NUM_EPISODES}, Recompensa Total: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\n",
    "                break\n",
    "\n",
    "        # Target model update\n",
    "        if (episode + 1) % TARGET_UPDATE_FREQUENCY == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        # Exploration rate decay\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "def test_agent(env, agent, episodes=5):\n",
    "    \"\"\"\n",
    "    Tests the agent in the environment.\n",
    "    \n",
    "    Args:\n",
    "        env (gym.Env): Environment.\n",
    "        agent (DQLAgent): Agent.\n",
    "        episodes (int): Number of episodes to run.\n",
    "    \"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "            q_values = agent.main_model.predict(np.array([state]), verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                print(f\"Prueba Episodio {episode + 1}/{episodes}, Recompensa Total: {total_reward}\")\n",
    "                break\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 1/5, Recompensa Total: 43.0, Epsilon: 1.00\n",
      "Episodio 2/5, Recompensa Total: 27.0, Epsilon: 0.99\n",
      "Episodio 3/5, Recompensa Total: 16.0, Epsilon: 0.99\n",
      "Episodio 4/5, Recompensa Total: 20.0, Epsilon: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodio 5/5, Recompensa Total: 19.0, Epsilon: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deloz\\anaconda3\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v0\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba Episodio 1/5, Recompensa Total: 9.0\n",
      "Prueba Episodio 2/5, Recompensa Total: 38.0\n",
      "Prueba Episodio 3/5, Recompensa Total: 34.0\n",
      "Prueba Episodio 4/5, Recompensa Total: 14.0\n",
      "Prueba Episodio 5/5, Recompensa Total: 22.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Environment\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Agent\n",
    "    agent = DQLAgent(state_size, action_size)\n",
    "\n",
    "    # Train agent\n",
    "    train_agent(env, agent)\n",
    "\n",
    "    # Save agent\n",
    "    # agent.save(\"cartpole-dql.h5\")\n",
    "\n",
    "    # Test agent\n",
    "    # test_agent(env, agent)\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
